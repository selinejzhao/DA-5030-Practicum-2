---
title: "R Notebook"
output: html_notebook
---
Problem 3

LOAD AND EXPLORE DATA:
```{r}
rm(list=ls())
setwd("/Users/selinezhao/Dropbox/_fall 2018/DA 5030/Practicum 2")
titanic <- read.csv("titanic_data.csv",header=TRUE,na.strings=c(""))
str(titanic)

summary(titanic)
```
As we can see, there is a lot of room for cleaning, especially in Name, Ticket, and Cabin. Embarked also has 2 rows with NA values.

CLEANING DATA:
Although the name column is mostly unusable for our purposes, there is one part within the feature that might be beneficial to us later -- the title.
```{r}
#create new Title feature extracted from Name
titanic$Title <- gsub('(.*, )|(\\..*)', '', titanic$Name)
table(titanic$Sex, titanic$Title)
```
Since there are quite a few different salutations being used throughout the dataset, but only a few popular ones, we will try and consolidate them as much as possible.
```{r}
#Mlle = Mademoiselle = Miss
#Ms = Miss
#Mme = Madame = Mrs
titanic$Title[titanic$Title == "Mlle"] <- "Miss" 
titanic$Title[titanic$Title == "Ms"] <- "Miss"
titanic$Title[titanic$Title == "Mme"] <- "Mrs"

#uncommon title counts combined to "rare" category since there aren't enough for us to draw conclusions from
rare_title <- c("Capt", "Col", "Don", "Dr", "Jonkheer", "Lady", "Major", "Rev", "Sir", "the Countess")
#if any of the titles are in the rare_title vector, replace with "Rare"
titanic$Title[titanic$Title %in% rare_title] <- "Rare"

#double check the Title column has been cleaned properly
table(titanic$Sex, titanic$Title)
```
In theory, the titles could be consolidated even further (i.e. Capt = Mr), however, some of these "rare" titles are indicative of class, so it's better to keep them separate.

IMPUTING MISSING DATA:
As we saw from the summary above, there are 2 rows with NA's from the embarked column. Since there are such few cases, it is worth it to impute so we can include this feature in future analysis methods.
```{r}
#identify which rows have missing values in Embarked
which(is.na(titanic$Embarked))
```
After doing some online research, it seems that the three embarkation points (Cherbourg, Queenstown, Southampton) depended on the passenger class, so it is reasonable to include the variables Pclass and Fare in the imputation process. 
```{r}
#create new dataframe for imputation process wtih relevant variables
titanic.emb <- titanic[c("Pclass", "Fare", "Embarked")]
head(titanic.emb)
```
```{r}
library(DMwR)
#knnImputation function from DMwR package, which fills in NA values in the dataframe using k Nearest Neighbors of each NA case
titanic.emb.imp <- knnImputation(titanic.emb, k = 30, scale = T, meth = "weighAvg", distData = NULL)
#df = titanic.emb
#k = 30, sqrt(891) = 29.8
#scale = TRUE, the data need be scaled before finding nearest neighbors since there is a large discrepancy between the ranges of the Pclass and Fare values
#meth = "weighAvg", this function will use a weighted average of the values of the neighbours. The weights are given by exp(-dist(k,x), where dist(k,x) is the euclidean distance between the case with NAs (x) and the neighbour k
#distData = NULL, because there are no training/test sets applicable
titanic.emb.imp[c(62, 830),]
```
We can see that the predicted embarkation point for both these cases is Cherbourg. This makes sense because both passengers are in 1st class and paid the same fare of 80 dollars. Furthermore, if we look at a box plot of these 3 variables, we will see that the median fare for a first class passenger departing from Cherbourg is also around 80 dollars.
```{r}
titanic.emb2 <- titanic.emb[-c(62,830),]
library(ggplot2)
ggplot(titanic.emb2, aes(x = Embarked, y = Fare, fill = factor(Pclass))) + geom_boxplot() + geom_hline(aes(yintercept=80), linetype="dashed")
```
```{r}
#insert imputed values into original dataset
titanic$Embarked[titanic$PassengerId == 62] <- "C"
titanic$Embarked[titanic$PassengerId == 830] <- "C"
sum(is.na(titanic$Embarked))
```
2. Impute any missing values for the age variable using an imputation strategy of your choice. State why you chose that strategy and what others could have been used and why you didn't choose them.
```{r}
sum(is.na(titanic$Age))
```
Because Age has quite a few data gaps, we will try to include as many features as possible to improve predictions, while still excluding any variables that aren't directly relevant or have missing data. The features we have deemed less-than-useful are Passenger Id, Name, Ticket, and Cabin.
```{r}
dropvars <- names(titanic) %in% c("PassengerId","Name","Ticket","Cabin")
titanic.age <- titanic[!dropvars]
titanic.age$Title <- as.factor(titanic.age$Title)
head(titanic.age)
```
Here, we will use kNN to impute the missing Age values because it is able to predict unknown values with no underlying assumptions of the training data distribution.Furthermore, it is effective with a variety of different data types, which is particularly useful for this situation since we are working with numeric, categorical, and other types of features. Naive Bayes is another powerful method for data imputation. However, it is not well suited for this specific instance because the algorithm does not work well with continuous variables, which are present in our dataset. Furthermore, Naive Bayes is primarily used for binary classification, and we are trying to impute a non-binary variable -- Age.
```{r}
titanic.age.imp <- knnImputation(titanic.age, k = 30, scale = T, meth = "weighAvg", distData = NULL)
head(titanic.age.imp)
```
```{r}
#round the entire age column
library(dplyr)
#mutate_at function from dplyr library, which allows the specified function to be applied to a selected group of columns within the dataframe
titanic.age.imp <- titanic.age.imp %>% mutate_at(4, round, 0)
#df = titanic.age.imp
# %>% = a method to "chain"/nest functions together
#mutate_at(col. # = 4, function = "round", # of digits = 0)
head(titanic.age.imp)
```
Let's compare the imputated results with the original distribution of passenger ages to ensure that nothing has gone completely awry.
```{r}
par(mfrow=c(1,2))
hist(titanic$Age, freq=F, main="Age: Original Data", xlab = "Age", col="green", ylim=c(0,0.04))
hist(titanic.age.imp$Age, freq=F, main="Age: KNN Imputated Data", xlab = "Age", col="blue", ylim=c(0,0.04))
```
Overall, the distribution has remained the same so we can apply the kNN age outputs to the original data.
```{r}
titanic$Age <- titanic.age.imp$Age
head(titanic)
sum(is.na(titanic$Age))
```
Now both the Age and Embarked features have been processed successfully. The only feature left with NAs is Cabin, which could potentially be imputed as well. However, given the sparseness of the column (over 75% of cells blanks), it would be more useful just to not include the feature during analysis.

BUILDING THE MODEL: 
Regression models generally only work with normally distributed data, so we need to double check that our continuous variables meet this criteria before doing anything else.
```{r}
library(psych)
pairs.panels(titanic[c("Age","Fare")])
```
As we can see, both feaatures could be improved through normalization, although the Age factor seems more normal in comparison to Fare.

Normalizing Age
```{r}
#histogram in detail
ggplot(titanic, aes(x=titanic$Age)) + geom_histogram(aes(y=..density..), bins=30, colour="black", fill="#84ee45")+geom_density(alpha=.2, fill="#bc15c1")
```
```{r}
#LOG TRANSFORM
ggplot(titanic, aes(x=log(titanic$Age))) + geom_histogram(aes(y=..density..),bins=30, colour="black", fill="#84ee45")+geom_density(alpha=.2, fill="#bc15c1")

#INVERSE TRANSFORM
ggplot(titanic, aes(x=1/((titanic$Age)))) + geom_histogram(aes(y=..density..),bins=30, colour="black", fill="#84ee45")+geom_density(alpha=.2, fill="#bc15c1")

#SQRT TRANSFORM
ggplot(titanic, aes(x=sqrt(titanic$Age))) + geom_histogram(aes(y=..density..),bins=30, colour="black", fill="#84ee45")+geom_density(alpha=.2, fill="#bc15c1")

#SQUARE TRANSFORM
ggplot(titanic, aes(x=(titanic$Age)^2)) + geom_histogram(aes(y=..density..),bins=30, colour="black", fill="#84ee45")+geom_density(alpha=.2, fill="#bc15c1")

par(mfrow=c(1,2))

#Z SCORE TRANSFORM
hist(((titanic$Age-mean(titanic$Age))/(sd(titanic$Age))), freq=F, main="Zscore", xlab = "Normalized Age", ylim = c(0,0.5))
lines(density((titanic$Age-mean(titanic$Age))/(sd(titanic$Age))), col="blue", lwd=2)

#MIN/MAX TRANSFORM
hist(((titanic$Age-min(titanic$Age))/(diff(range(titanic$Age)))), freq=F, main = "Min/Max", xlab = "Normalized Age")
lines(density((titanic$Age-min(titanic$Age))/(diff(range(titanic$Age)))), col="blue", lwd=2)
```
None of the transforms significantly affect the distribution, so we are going to leave the original.

Normalizing Fare
```{r}
ggplot(titanic, aes(x=titanic$Fare)) + geom_histogram(aes(y=..density..), bins=30, colour="black", fill="#84ee45")+geom_density(alpha=.2, fill="#bc15c1")
```
```{r}
#LOG TRANSFORM
ggplot(titanic, aes(x=log(titanic$Fare))) + geom_histogram(aes(y=..density..),bins=30, colour="black", fill="#84ee45")+geom_density(alpha=.2, fill="#bc15c1")

#INVERSE TRANSFORM
ggplot(titanic, aes(x=1/((titanic$Fare)))) + geom_histogram(aes(y=..density..),bins=30, colour="black", fill="#84ee45")+geom_density(alpha=.2, fill="#bc15c1")

#SQRT TRANSFORM
ggplot(titanic, aes(x=sqrt(titanic$Fare))) + geom_histogram(aes(y=..density..),bins=30, colour="black", fill="#84ee45")+geom_density(alpha=.2, fill="#bc15c1")

#SQUARE TRANSFORM
ggplot(titanic, aes(x=(titanic$Fare)^2)) + geom_histogram(aes(y=..density..),bins=30, colour="black", fill="#84ee45")+geom_density(alpha=.2, fill="#bc15c1")

par(mfrow=c(1,2))

#Z SCORE TRANSFORM
hist(((titanic$Fare-mean(titanic$Fare))/(sd(titanic$Fare))), freq=F, main="Zscore", xlab = "Normalized Age", ylim = c(0,0.5))
lines(density((titanic$Fare-mean(titanic$Fare))/(sd(titanic$Fare))), col="blue", lwd=2)

#MIN/MAX TRANSFORM
hist(((titanic$Fare-min(titanic$Fare))/(diff(range(titanic$Fare)))), freq=F, main = "Min/Max", xlab = "Normalized Age")
lines(density((titanic$Fare-min(titanic$Fare))/(diff(range(titanic$Fare)))), col="blue", lwd=2)
```
Although none of these transforms are quite ideal, the square root transform is an improvement, so we will replace all cases of Fare with its square root. However, it is important to note that no matter what transform we apply to this data, the number "0" values will always be present which will likely skew the data a bit.
```{r}
titanic.norm <- titanic
titanic.norm$Fare <- sqrt(titanic.norm$Fare)
head(titanic$Fare)
head(titanic.norm$Fare)
```
1. Divide the provided Titanic Survival Data into two subsets: a training data set and a test data set. Use whatever strategy you believe it best. Justify your answer.

Because the Titanic dataset is already in a random order, we can split the set 80/20 train/test.
```{r}
titanic.norm$Survived <- as.factor(titanic.norm$Survived)
t.train <- titanic.norm[1:712,]
t.test <- titanic.norm[713:891,]
prop.table(table(t.train$Survived))
prop.table(table(t.test$Survived))
```
Both training and test datasets have an evenly split survival rate, so we can start building our model.

3. Construct a logistic regression model to predict the probability of a passenger surviving the Titanic accident. Test the statistical significance of all parameters and eliminate those that have a p-value > 0.05 using stepwise backward elimination.
```{r}
model <- glm(formula = Survived ~ Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+Title, family=binomial, data = t.train)
summary(model)
```
As shown above, we can see that TitleMrs isn't statistically significant, so we will remove it. This is an automatically created dummy variable, so we will remove it by excluding it from the other Title dummies. 
```{r}
model2 <- glm(formula = Survived ~ Pclass+Sex+Age+SibSp+Parch+Fare+Embarked+factor(Title, exclude="Mrs"), family=binomial, data = t.train)
summary(model2)
```
Sex to be removed next:
```{r}
model3 <- glm(formula = Survived ~ Pclass+Age+SibSp+Parch+Fare+Embarked+factor(Title, exclude="Mrs"), family=binomial, data = t.train)
summary(model3)
```
EmbarkedQ to be removed next:
```{r}
model4 <- glm(formula = Survived ~ Pclass+Age+SibSp+Parch+Fare+factor(Embarked, exclude = "Q")+factor(Title, exclude="Mrs"), family=binomial, data = t.train)
summary(model4)
```
Parch to be removed next:
```{r}
model5 <- glm(formula = Survived ~ Pclass+Age+SibSp+Fare+factor(Embarked, exclude = "Q")+factor(Title, exclude="Mrs"), family=binomial, data = t.train)
summary(model5)
```
Fare to be removed next:
```{r}
model6 <- glm(formula = Survived ~ Pclass+Age+SibSp+factor(Embarked, exclude = "Q")+factor(Title, exclude="Mrs"), family=binomial, data = t.train)
summary(model6)
```
TitleMiss to be removed next:
```{r}
model7 <- glm(formula = Survived ~ Pclass+Age+SibSp+factor(Embarked, exclude = "Q")+factor(Title, exclude=c("Mrs", "Miss")), family=binomial, data = t.train)
summary(model7)
```
Now remove the rest of Embarked:
```{r}
model.final <- glm(formula = Survived ~ Pclass+Age+SibSp+factor(Title, exclude=c("Mrs", "Miss")), family=binomial, data = t.train)
summary(model.final)
```
Now all of our predictors are statistically significant! The final model includes Pclass, Age, SibSp, TitleMr, TitleRare.

Let's analyze the model. As we can see the final model AIC is 398.06, which seems pretty good! The model features are statistically significant and contribute information to predicting the survived class. The residuals tell us how much our fitted values are off of actual values for each case. Our median residual is only -0.4884 which is not bad for our purposes.

4. State the model as a regression equation.
The regression equation is Y = 4.57789 - 0.92731(Pclass) - 0.03136(Age) - 0.3978(SibSp) - 2.95276(TitleMr) - 2.08464(TitleRare)

EVALUATING THE MODEL:
5. Test the model against the test data set and determine its prediction accuracy (as a percentage correct).
```{r}
#predict actual values
pred <- predict(model.final, t.test, type = "response")
#apply prediction threshold
pred <- as.factor(ifelse(pred > 0.5,1,0))
#check accuracy and false/true positives/negatives
library(caret)
confusionMatrix(pred, t.test$Survived)
```
As we can see, the overall model p-value is less than 0.05, which is great! Also the accuracy is 87.8%. This is pretty good. We had 2 false positives and 13 false negatives. In this instance, a false positive would be more costly because predicting someone to survive when they actually do not is misleading. Luckily we only had 2 of these. 

*References*
[1] Semidevil, “how to extract titles out of a full name using (gsub),” Talk Stats, 15-May-2017. [Online]. Available: http://www.talkstats.com/threads/how-to-extract-titles-out-of-a-full-name-using-gsub.69337/. [Accessed: 05-Nov-2018].

[2] “Basic Feature Engineering with the Titanic Data,” triangleinequality, 08-Sep-2013. [Online]. Available: https://triangleinequality.wordpress.com/2013/09/08/basic-feature-engineering-with-the-titanic-data/. [Accessed: 05-Nov-2018].

[3] “RMS Titanic,” Wikipedia, 07-Nov-2018. [Online]. Available: https://en.wikipedia.org/wiki/RMS_Titanic. [Accessed: 05-Nov-2018].

[4] S. Prabhakaran, “Missing Value Treatment,” DataScience , 25-Apr-2016. [Online]. Available: https://datascienceplus.com/missing-value-treatment/. [Accessed: 05-Nov-2018].

[5] “DMwR,” function | R Documentation. [Online]. Available: https://www.rdocumentation.org/packages/DMwR/versions/0.4.1/topics/knnImputation. [Accessed: 05-Nov-2018].

[6] Titanic: Machine Learning from Disaster. [Online]. Available: https://www.kaggle.com/c/titanic. [Accessed: 05-Nov-2018].

[7] “ggplot2 box plot : Quick start guide - R software and data visualization,” Correlation matrix - STHDA. [Online]. Available: http://www.sthda.com/english/wiki/ggplot2-box-plot-quick-start-guide-r-software-and-data-visualization#basic-box-plots. [Accessed: 05-Nov-2018].

[8] “How to use dplyr::mutate_all for rounding selected columns,” Stack Overflow. [Online]. Available: https://stackoverflow.com/questions/43314328/how-to-use-dplyrmutate-all-for-rounding-selected-columns. [Accessed: 05-Nov-2018].
